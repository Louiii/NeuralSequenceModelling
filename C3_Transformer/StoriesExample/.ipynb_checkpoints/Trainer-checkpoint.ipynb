{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Trainer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn, torch.optim as optim\n",
    "from ParameterPartitioner import multiple_partition, apply_initialisation, optimisation_groups\n",
    "from Model import TransformerModel\n",
    "\n",
    "ntoken = 10\n",
    "ninp = 8\n",
    "nhead = 4\n",
    "nhid = 7\n",
    "nlayers = 2\n",
    "dropout = 0.1\n",
    "\n",
    "model = TransformerModel(ntoken, ninp, nhead, nhid, nlayers, dropout)\n",
    "# print([pn for pn, p in model.named_modules()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialise the model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print([str(m).split('(')[0] for m in model.modules()])\n",
    "\n",
    "weight_kwargs = {'Embedding':(nn.init.normal_, [], {'mean':0, 'std':0.02}),\n",
    "                 'Linear':(nn.init.normal_, [], {'mean':0, 'std':0.02}),\n",
    "                 'LayerNorm':(nn.init.constant_, [1], {})}\n",
    "bias_kwargs =   {'Default':(nn.init.constant_, [0], {})}\n",
    "apply_initialisation(model, (weight_kwargs, bias_kwargs))#, pr=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Give parameters different optimisers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 3e-4\n",
    "betas = (0.9, 0.95)\n",
    "conditions = ['weight', 'linear']\n",
    "combine = {'linear weights':[[1,1]], 'other':None}\n",
    "typ_opt_kwargs = {'linear weights': {\"weight_decay\": 0.1},\n",
    "                  'other': {\"weight_decay\": 0.0}}\n",
    "\n",
    "partition = multiple_partition(model, conditions, comb=combine)\n",
    "opt_groups = optimisation_groups(model, partition, typ_opt_kwargs)\n",
    "\n",
    "# for k, v in partition.items(): print('\\n'+k+'\\n%s'%v)\n",
    "optimiser = torch.optim.AdamW(opt_groups, lr=lr, betas=betas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "model.train() tells your model that you are training the model. So effectively layers like dropout, batchnorm etc. which behave different on the train and test procedures know what is going on and hence can behave accordingly.\n",
    "\n",
    "More details: It sets the mode to train (see source code). You can call either"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerModel(\n",
       "  (pos_encoder): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): Linear(in_features=8, out_features=8, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=8, out_features=7, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=7, out_features=8, bias=True)\n",
       "        (norm1): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): Linear(in_features=8, out_features=8, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=8, out_features=7, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=7, out_features=8, bias=True)\n",
       "        (norm1): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (encoder): Embedding(10, 8)\n",
       "  (decoder): Linear(in_features=8, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
